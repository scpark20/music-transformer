{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total data files :  1282\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "data_dir = 'dataset/'\n",
    "\n",
    "data_files = []\n",
    "\n",
    "data_files = [join(data_dir, f) for f in listdir(data_dir) if isfile(join(data_dir, f)) if '.npz' in f]\n",
    "\n",
    "data_files.sort()\n",
    "\n",
    "print('total data files : ', len(data_files))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters declaration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "IntervalDim = 100\n",
    "\n",
    "VelocityDim = 32\n",
    "VelocityOffset = IntervalDim\n",
    "\n",
    "NoteOnDim = NoteOffDim = 128\n",
    "NoteOnOffset = IntervalDim + VelocityDim\n",
    "NoteOffOffset = IntervalDim + VelocityDim + NoteOnDim\n",
    "\n",
    "EventDim = IntervalDim + VelocityDim + NoteOnDim + NoteOffDim # 388\n",
    "\n",
    "Time = 650\n",
    "\n",
    "EmbeddingDim = 512\n",
    "\n",
    "HeadDim = 32\n",
    "Heads = 16\n",
    "ContextDim = HeadDim * Heads # 512\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape :  (650,)\n",
      "y shape :  (650,)\n",
      "[ 18 172 214   4 342  15 169 211   4 339   4 168 212   5 340   5 171 211\n",
      "   8 339   1 163 212   7 183 214   1 340   2 342  18 179 211   5 339  16\n",
      " 188 183   0 172 171   1 188 207   3 311   1 299  31 335   7 196 195   1\n",
      " 183 183  32 323   3 311   2 186 196   0 185 208   3 324   3 336  32 175\n",
      " 181   0 181 193   0 197 208   0 198 220   6 321   0 309  32 172 179   1\n",
      " 336   0 188 191   1 348   3 307   1 319  35 184 219   0 195 207   1 196\n",
      " 203   2 182 191   3 335   0 347  33 319   1 331   0 195 205   5 333  32\n",
      " 189 202   0 195 205   0 175 190   0 192 217   3 330   5 318  30 345   0\n",
      " 333   1 178 176   0 190 188   5 304   0 316  33 195 203   1 186 200   0\n",
      " 189 215   2 157 188   1 145 187   2 331   1 343   4 315  26 328   1 316\n",
      "   1 189 202   4 330  35 175 198   0 192 202   0 189 214   0 170 186   3\n",
      " 326   5 314  30 177 173   0 187 185   1 342   1 330   3 313   1 301  36\n",
      " 183 212   0 186 200   1 188 197   1 174 185   5 328   0 340  31 189 198\n",
      "   3 325   2 326   2 313  31 188 198   0 158 183   0 175 210   0 159 195\n",
      "   5 326   1 338  26 323   7 180 197   6 311   0 325  33 177 188   0 189\n",
      " 197   0 175 183   0 177 209   0 167 176   1 171 207   7 325   2 335   0\n",
      " 316   1 304   2 337   2 311  23 187 199   9 327  29 188 190   0 193 199\n",
      "   0 186 211   0 185 207   0 179 183   1 167 178   6 306   1 318   1 327\n",
      "   3 335   1 311   1 339  26 196 200  10 328  29 177 179   0 204 200   0\n",
      " 197 191   0 188 212   0 190 207   0 185 183   4 148 210   2 122 202   0\n",
      " 330   1 338   0 340   1 328   0 319   1 307   1 335   1 311  28 192 198\n",
      "   8 326  28 175 177   0 199 189   0 198 198   1 181 207   0 192 210   0\n",
      " 181 183   6 305   0 317   1 326   0 338   0 311   1 335  43 154 206   0\n",
      " 145 190   1 151 197   1 157 209   2 139 178  28 325   2 337   2 334  47\n",
      " 162 189   1 318   5 317   2 148 190   6 318   4 164 189   9 317   1 154\n",
      " 190   7 167 192   0 318   4 320  17 164 189   4 317  19 142 186   2 157\n",
      " 201   1 151 195  38 314   0 329   3 148 185  38 155 201   6 329   1 166\n",
      " 202   8 330   5 162 201   7 323   3 329   2 159 202   5 330   1 160 204\n",
      "   5 332  14 306   1 165 201   4 329  14 313   3 143 190   1 117 178   0\n",
      " 164 202   0 154 194  15 322  26 162 206   1 160 197   0 330   3 334   2\n",
      " 325  12 318  24 166 189   6 317   2 165 190   7 318   3 164 189   9 317\n",
      "   1 153 190   6 318   0 165 192   4 320  15 165 189   4 317  22 163 204\n",
      "   1 160 195   0 141 186   0 158 201  33 314   7 160 185   2 332   2 323\n",
      "  12 329  23 160 204   5 332   2 162 206   5 334   3 167 204  12 332   4\n",
      " 160 206   5 334   0 167 207   3 335  15 306   2 164 204   8 332   8 313\n",
      "   6 143 190   1 131 178   1 158 197   1 161 206   0 160 202  13 334  24\n",
      " 325   5]\n",
      "[172 214   4 342  15 169 211   4 339   4 168 212   5 340   5 171 211   8\n",
      " 339   1 163 212   7 183 214   1 340   2 342  18 179 211   5 339  16 188\n",
      " 183   0 172 171   1 188 207   3 311   1 299  31 335   7 196 195   1 183\n",
      " 183  32 323   3 311   2 186 196   0 185 208   3 324   3 336  32 175 181\n",
      "   0 181 193   0 197 208   0 198 220   6 321   0 309  32 172 179   1 336\n",
      "   0 188 191   1 348   3 307   1 319  35 184 219   0 195 207   1 196 203\n",
      "   2 182 191   3 335   0 347  33 319   1 331   0 195 205   5 333  32 189\n",
      " 202   0 195 205   0 175 190   0 192 217   3 330   5 318  30 345   0 333\n",
      "   1 178 176   0 190 188   5 304   0 316  33 195 203   1 186 200   0 189\n",
      " 215   2 157 188   1 145 187   2 331   1 343   4 315  26 328   1 316   1\n",
      " 189 202   4 330  35 175 198   0 192 202   0 189 214   0 170 186   3 326\n",
      "   5 314  30 177 173   0 187 185   1 342   1 330   3 313   1 301  36 183\n",
      " 212   0 186 200   1 188 197   1 174 185   5 328   0 340  31 189 198   3\n",
      " 325   2 326   2 313  31 188 198   0 158 183   0 175 210   0 159 195   5\n",
      " 326   1 338  26 323   7 180 197   6 311   0 325  33 177 188   0 189 197\n",
      "   0 175 183   0 177 209   0 167 176   1 171 207   7 325   2 335   0 316\n",
      "   1 304   2 337   2 311  23 187 199   9 327  29 188 190   0 193 199   0\n",
      " 186 211   0 185 207   0 179 183   1 167 178   6 306   1 318   1 327   3\n",
      " 335   1 311   1 339  26 196 200  10 328  29 177 179   0 204 200   0 197\n",
      " 191   0 188 212   0 190 207   0 185 183   4 148 210   2 122 202   0 330\n",
      "   1 338   0 340   1 328   0 319   1 307   1 335   1 311  28 192 198   8\n",
      " 326  28 175 177   0 199 189   0 198 198   1 181 207   0 192 210   0 181\n",
      " 183   6 305   0 317   1 326   0 338   0 311   1 335  43 154 206   0 145\n",
      " 190   1 151 197   1 157 209   2 139 178  28 325   2 337   2 334  47 162\n",
      " 189   1 318   5 317   2 148 190   6 318   4 164 189   9 317   1 154 190\n",
      "   7 167 192   0 318   4 320  17 164 189   4 317  19 142 186   2 157 201\n",
      "   1 151 195  38 314   0 329   3 148 185  38 155 201   6 329   1 166 202\n",
      "   8 330   5 162 201   7 323   3 329   2 159 202   5 330   1 160 204   5\n",
      " 332  14 306   1 165 201   4 329  14 313   3 143 190   1 117 178   0 164\n",
      " 202   0 154 194  15 322  26 162 206   1 160 197   0 330   3 334   2 325\n",
      "  12 318  24 166 189   6 317   2 165 190   7 318   3 164 189   9 317   1\n",
      " 153 190   6 318   0 165 192   4 320  15 165 189   4 317  22 163 204   1\n",
      " 160 195   0 141 186   0 158 201  33 314   7 160 185   2 332   2 323  12\n",
      " 329  23 160 204   5 332   2 162 206   5 334   3 167 204  12 332   4 160\n",
      " 206   5 334   0 167 207   3 335  15 306   2 164 204   8 332   8 313   6\n",
      " 143 190   1 131 178   1 158 197   1 161 206   0 160 202  13 334  24 325\n",
      "   5 163]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1296x720 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_data(length=Time):\n",
    "    index = np.random.randint(0, len(data_files))\n",
    "    data = np.load(data_files[index])['eventlist']\n",
    "    \n",
    "    # time augmentation\n",
    "    data[:, 0] *= np.random.uniform(0.95, 1.05)\n",
    "    \n",
    "    # absolute time to relative interval\n",
    "    data[1:, 0] = data[1:, 0] - data[:-1, 0]\n",
    "    data[0, 0] = 0\n",
    "    \n",
    "    # discretize interval into IntervalDim\n",
    "    data[:, 0] = np.clip(np.round(data[:, 0] * IntervalDim), 0, IntervalDim - 1)\n",
    "    \n",
    "    # Note augmentation\n",
    "    data[:, 2] += np.random.randint(-6, 6)\n",
    "    data[:, 2] = np.clip(data[:, 2], 0, NoteOnDim)\n",
    "    \n",
    "    eventlist = []\n",
    "    for d in data:\n",
    "        # append interval\n",
    "        interval = d[0]\n",
    "        eventlist.append(interval)\n",
    "    \n",
    "        # note on case\n",
    "        if d[1] == 1:\n",
    "            velocity = d[3] + VelocityOffset\n",
    "            note = d[2] + NoteOnOffset\n",
    "            eventlist.append(velocity)\n",
    "            eventlist.append(note)\n",
    "            \n",
    "        # note off case\n",
    "        elif d[1] == 0:\n",
    "            note = d[2] + NoteOffOffset\n",
    "            eventlist.append(note)\n",
    "            \n",
    "    eventlist = np.array(eventlist).astype(np.int)\n",
    "    \n",
    "    if len(eventlist) > (length+1):\n",
    "        start_index = np.random.randint(0, len(eventlist) - (length+1))\n",
    "        eventlist = eventlist[start_index:start_index+(length+1)]\n",
    "        \n",
    "    # pad zeros\n",
    "    if len(eventlist) < (length+1):\n",
    "        pad = (length+1) - len(eventlist)\n",
    "        eventlist = np.pad(eventlist, (pad, 0), 'constant')\n",
    "        \n",
    "    x = eventlist[:length]\n",
    "    y = eventlist[1:length+1]\n",
    "    \n",
    "    return x, y\n",
    "    \n",
    "x, y = get_data()\n",
    "print('x shape : ', x.shape)\n",
    "print('y shape : ', y.shape)\n",
    "print(x)\n",
    "print(y)\n",
    "    \n",
    "    \n",
    "roll = np.zeros([len(x), EventDim])\n",
    "for t, _x in enumerate(x):\n",
    "    roll[t, _x] = 1\n",
    "\n",
    "plt.figure(figsize=[18, 10])\n",
    "librosa.display.specshow(roll.T)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# thanks to @openai: https://github.com/openai/gpt-2/blob/master/src/model.py\n",
    "def shape_list(x):\n",
    "    \"\"\"Deal with dynamic shape in tensorflow cleanly.\"\"\"\n",
    "    static = x.shape.as_list()\n",
    "    dynamic = tf.shape(x)\n",
    "    return [dynamic[i] if s is None else s for i, s in enumerate(static)]\n",
    "\n",
    "def model(inputs):\n",
    "    with tf.variable_scope('model'):\n",
    "        # inputs : [Batch, Time, EmbeddingDim]\n",
    "        \n",
    "        Batch, _, _ = shape_list(inputs)\n",
    "        \n",
    "        # [Heads, Batch, Time, HeadDim]\n",
    "        Q = tf.stack([tf.layers.dense(inputs, HeadDim, use_bias=False) for _ in range(Heads)])\n",
    "        K = tf.stack([tf.layers.dense(inputs, HeadDim, use_bias=False) for _ in range(Heads)])\n",
    "        V = tf.stack([tf.layers.dense(inputs, HeadDim, use_bias=False) for _ in range(Heads)])\n",
    "        \n",
    "        '''\n",
    "        [E_(-T+1), ..., E_0]\n",
    "        '''\n",
    "        E = tf.get_variable('E', [Heads, Time, HeadDim])\n",
    "        \n",
    "        # [Heads, Batch * Time, HeadDim]\n",
    "        Q_ = tf.reshape(Q, [Heads, Batch * Time, HeadDim])\n",
    "        \n",
    "        # [Heads, Batch * Time, Time]\n",
    "        S = tf.matmul(Q_, E, transpose_b=True)\n",
    "        # [Heads, Batch, Time, Time]\n",
    "        S = tf.reshape(S, [Heads, Batch, Time, Time])\n",
    "        # [Heads, Batch, Time, Time+1]\n",
    "        S = tf.pad(S, ((0, 0), (0, 0), (0, 0), (1, 0)))\n",
    "        # [Heads, Batch, Time+1, Time]\n",
    "        S = tf.reshape(S, [Heads, Batch, Time+1, Time])\n",
    "        # [Heads, Batch, Time, Time]\n",
    "        S = S[:, :, 1:]\n",
    "        \n",
    "        # [Heads, Batch, Time, Time]\n",
    "        attention = (tf.matmul(Q, K, transpose_b=True) + S) / np.sqrt(HeadDim)\n",
    "        \n",
    "        '''\n",
    "        # [Time, Time]  [[1 0 0 0]\n",
    "                         [1 1 0 0]\n",
    "                         [1 1 1 0]\n",
    "                         [1 1 1 1]]\n",
    "        '''\n",
    "        mask = tf.matrix_band_part(tf.ones([Time, Time]), -1, 0)\n",
    "        \n",
    "        # [Heads, Batch, Time, Time]\n",
    "        attention = attention * mask - tf.cast(1e10, attention.dtype) * (1-mask)\n",
    "        # [Heads, Batch, Time, Time]\n",
    "        score = tf.nn.softmax(attention, axis=3)\n",
    "        \n",
    "        # [Heads, Batch, Time, HeadDim]\n",
    "        context = tf.matmul(score, V)\n",
    "        # [Batch, Time, Heads, HeadDim]\n",
    "        context = tf.transpose(context, [1, 2, 0, 3])\n",
    "        # [Batch, Time, ContextDim]\n",
    "        context = tf.reshape(context, [Batch, Time, ContextDim])\n",
    "        # [Batch, Time, ContextDim]\n",
    "        context = tf.layers.dense(context, ContextDim, tf.nn.relu)\n",
    "        # [Batch, Time, EventDim]\n",
    "        logits = tf.layers.dense(context, EventDim)\n",
    "        \n",
    "        return logits\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Draw Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/scpark/anaconda3/envs/ai/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From <ipython-input-7-2ad2e6614e8f>:15: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "WARNING:tensorflow:From <ipython-input-8-537ce8537aed>:30: Categorical.__init__ (from tensorflow.python.ops.distributions.categorical) is deprecated and will be removed after 2019-01-01.\n",
      "Instructions for updating:\n",
      "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n",
      "WARNING:tensorflow:From /home/scpark/anaconda3/envs/ai/lib/python3.6/site-packages/tensorflow/python/ops/distributions/categorical.py:242: Distribution.__init__ (from tensorflow.python.ops.distributions.distribution) is deprecated and will be removed after 2019-01-01.\n",
      "Instructions for updating:\n",
      "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n",
      "WARNING:tensorflow:From /home/scpark/anaconda3/envs/ai/lib/python3.6/site-packages/tensorflow/python/ops/distributions/categorical.py:278: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.random.categorical instead.\n",
      "WARNING:tensorflow:From /home/scpark/anaconda3/envs/ai/lib/python3.6/site-packages/tensorflow/python/ops/array_grad.py:425: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "graph created\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.reset_default_graph()\n",
    "\n",
    "'''INPUTS'''\n",
    "# [Batch, Time]\n",
    "inputs = tf.placeholder(dtype=tf.int32, shape=[None, Time])\n",
    "# [Batch, Time]\n",
    "targets = tf.placeholder(dtype=tf.int32, shape=[None, None])\n",
    "\n",
    "# [Batch, Time, EventDim], for the use of visualization\n",
    "inputs_onehot = tf.one_hot(inputs, axis=2, depth=EventDim)\n",
    "\n",
    "'''EMBEDDING'''\n",
    "embedding = tf.get_variable('embedding', [EventDim, EmbeddingDim])\n",
    "# [Batch, Time, EventDim]\n",
    "inputs_embedding = tf.gather(embedding, inputs)\n",
    "\n",
    "'''GET LOGITS'''\n",
    "# [Batch, Time, EventDim]\n",
    "logits = model(inputs_embedding)\n",
    "\n",
    "# for the use of visualization\n",
    "probs = tf.nn.softmax(logits, axis=2)\n",
    "\n",
    "'''LOSS'''\n",
    "cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=targets, logits=logits)\n",
    "loss = tf.reduce_mean(cross_entropy)\n",
    "\n",
    "'''SAMPLING'''\n",
    "dist = tf.distributions.Categorical(logits=logits[:, -1])\n",
    "# [Batch, 1]\n",
    "sample = dist.sample()\n",
    "\n",
    "'''TRAIN'''\n",
    "global_step = tf.Variable(0, name='global_step')\n",
    "learning_rate = tf.Variable(1e-3, name='learning_rate')\n",
    "train_step = tf.train.AdamOptimizer(learning_rate).minimize(loss, global_step)\n",
    "\n",
    "'''SESSION OPEN'''\n",
    "\n",
    "sess_config = tf.ConfigProto()\n",
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.9, visible_device_list= '0')\n",
    "sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "print('graph created')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model not exist.\n"
     ]
    }
   ],
   "source": [
    "load_dir = 'save/music-transformer'\n",
    "save_dir = 'save/music-transformer'\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "if True:\n",
    "    restore_file = tf.train.latest_checkpoint(load_dir)\n",
    "    if restore_file is not None:\n",
    "        saver.restore(sess, restore_file)\n",
    "        print(\"Model restored.\")\n",
    "    else:\n",
    "        print('model not exist.')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "class Logger(SummaryWriter):\n",
    "    def __init__(self, logdir):\n",
    "        super(Logger, self).__init__(logdir)\n",
    "\n",
    "    def log(self, log_string, value, iteration):\n",
    "            self.add_scalar(log_string, value, iteration)\n",
    "            \n",
    "logger = Logger(save_dir)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 650) (8, 650)\n",
      "step :  1399 loss :  3.9531817\n",
      "(8, 650) (8, 650)\n",
      "step :  1400 loss :  3.77027\n",
      "(8, 650) (8, 650)\n",
      "step :  1401 loss :  3.7222223\n",
      "(8, 650) (8, 650)\n",
      "step :  1402 loss :  3.7977307\n",
      "(8, 650) (8, 650)\n",
      "step :  1403 loss :  3.9086936\n",
      "(8, 650) (8, 650)\n",
      "step :  1404 loss :  3.835322\n",
      "(8, 650) (8, 650)\n",
      "step :  1405 loss :  3.847772\n",
      "(8, 650) (8, 650)\n",
      "step :  1406 loss :  3.752189\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa.display\n",
    "from time import sleep\n",
    "import time\n",
    "\n",
    "batch_size = 8\n",
    "def get_batch_data(batch_size, time):\n",
    "    _inputs = []\n",
    "    _targets = []\n",
    "    for _ in range(batch_size):\n",
    "        x, y = get_data(time)\n",
    "        _inputs.append(x)\n",
    "        _targets.append(y)\n",
    "        \n",
    "    _inputs = np.stack(_inputs)\n",
    "    _targets = np.stack(_targets)\n",
    "    \n",
    "    return _inputs, _targets\n",
    "    \n",
    "while(True):\n",
    "    for _ in range(100):\n",
    "        _inputs, _targets = get_batch_data(batch_size, Time)\n",
    "        print(_inputs.shape, _targets.shape)\n",
    "        \n",
    "        _, _global_step, _loss = sess.run([train_step, global_step, loss], \n",
    "                                          feed_dict={inputs: _inputs, \n",
    "                                                     targets: _targets,\n",
    "                                                     learning_rate: 1e-3})\n",
    "        print('step : ', _global_step, 'loss : ', _loss)\n",
    "        \n",
    "        if _global_step % 10 == 0:\n",
    "            logger.log('loss', _loss, _global_step)\n",
    "        \n",
    "        if _global_step % 1000 == 0:\n",
    "            save_path = saver.save(sess, save_dir + '/checkpoint', global_step=_global_step)\n",
    "            print(\"Model saved in path: %s\" % save_path)\n",
    "        \n",
    "    clear_output()\n",
    "    \n",
    "    _inputs_onehot, _probs = sess.run([inputs_onehot, probs], feed_dict={inputs: _inputs})\n",
    "    \n",
    "    plt.figure(figsize=[18, 10])\n",
    "    librosa.display.specshow(_inputs_onehot[0].T)\n",
    "    plt.show()\n",
    "    \n",
    "    plt.figure(figsize=[18, 10])\n",
    "    librosa.display.specshow(_probs[0].T)\n",
    "    plt.show()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
