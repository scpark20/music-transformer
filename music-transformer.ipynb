{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total data files :  1282\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "data_dir = 'dataset/'\n",
    "\n",
    "data_files = []\n",
    "\n",
    "data_files = [join(data_dir, f) for f in listdir(data_dir) if isfile(join(data_dir, f)) if '.npz' in f]\n",
    "\n",
    "data_files.sort()\n",
    "\n",
    "print('total data files : ', len(data_files))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters declaration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "IntervalDim = 100\n",
    "\n",
    "VelocityDim = 32\n",
    "VelocityOffset = IntervalDim\n",
    "\n",
    "NoteOnDim = NoteOffDim = 128\n",
    "NoteOnOffset = IntervalDim + VelocityDim\n",
    "NoteOffOffset = IntervalDim + VelocityDim + NoteOnDim\n",
    "\n",
    "EventDim = NoteOnDim + NoteOffDim + IntervalDim + VelocityDim # 388\n",
    "\n",
    "Time = 650\n",
    "\n",
    "EmbeddingDim = 512\n",
    "\n",
    "HeadDim = 32\n",
    "Heads = 16\n",
    "ContextDim = HeadDim * Heads # 512\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape :  (650,)\n",
      "y shape :  (650,)\n",
      "[  3 260   5 198 133   1 171 133   2 260   1 260   2 198 133   4 260   4\n",
      " 193 133   1 185 133   5 260   1 196 133   0 260   3 260   6 193 133   0\n",
      " 185 133   4 260   2 260   1 200 133   5 260   3 195 133   1 185 133   3\n",
      " 260   0 260   4 195 133   2 260   3 189 133   1 186 133   2 260   2 141\n",
      " 133   1 260   0 199 133   2 260   0 260  36 208 133   1 202 133   0 201\n",
      " 133   0 200 133   2 260   2 260   8 260   9 260  30 202 133   1 203 133\n",
      "   1 187 133   0 175 133   2 163 133   1 260   0 260   1 260   1 260   1\n",
      " 260   4 161 133   3 260   1 177 133   2 146 133   5 260   1 167 133   2\n",
      " 161 133   2 260   1 260   3 172 133   2 260   2 260   1 171 133   1 179\n",
      " 133   2 260   3 173 133   2 260   1 260   1 159 133   2 177 133   3 176\n",
      " 133   1 260   2 158 133   1 260   0 260   2 175 133   2 260   2 260   3\n",
      " 169 133   0 178 133   4 260   0 166 133   1 260   2 167 133   1 178 133\n",
      "   1 260   0 171 133   0 260   2 178 133   1 260   3 260   1 177 133   2\n",
      " 260   1 260   2 201 133   0 199 133   4 260   0 260   2 195 133   6 260\n",
      "   3 193 133   1 183 133   3 260   0 260   3 198 133   4 260   3 194 133\n",
      "   0 187 133   5 260   0 260   1 188 133   2 260   5 200 133   0 192 133\n",
      "   5 260   1 260   2 196 133   5 260   3 193 133   0 190 133   3 260   0\n",
      " 260   3 203 133   4 260   3 201 133   1 194 133   4 260   1 260   2 200\n",
      " 133   5 260   3 198 133   2 178 133   3 260   0 260   3 206 133   4 260\n",
      "   2 202 133   0 177 133   3 260   1 260   3 204 133   3 260  38 208 133\n",
      "   1 202 133   1 198 133   0 206 133   1 260   1 260   0 260   0 260   1\n",
      " 152 133   0 260   0 113 133   3 260  40 199 133   1 202 133   1 201 133\n",
      "   2 141 133   1 260   1 260   1 260   0 163 133   1 260   3 260   6 148\n",
      " 133   1 155 133   2 165 133   1 260   3 260   0 166 133   2 260   2 260\n",
      "   3 169 133   1 175 133   3 260   3 174 133   2 260   1 176 133   1 260\n",
      "   0 171 133   6 182 133   0 260   1 161 133   2 260   1 167 133   0 260\n",
      "   2 260   1 260   2 176 133   2 174 133   5 260   1 172 133   1 260   2\n",
      " 169 133   1 171 133   3 165 133   2 260   0 167 133   1 260   0 260   0\n",
      " 174 133   2 260   1 260   0 260   4 177 133   4 260   6 183 133   1 185\n",
      " 133   2 260   1 260   4 182 133   3 260   4 184 133   0 182 133   3 260\n",
      "   0 260   5 176 133   2 260   2 170 133   2 180 133   1 260   2 260   6\n",
      " 177 133   3 260   2 186 133   0 193 133   3 260   0 153 133   1 260   2\n",
      " 260   4 199 133   5 190 133   0 260   0 186 133   2 158 133   0 260   0\n",
      " 260   0 260   5 193 133   4 194 133   0 260   0 190 133   2 260   3 260\n",
      "   6 170 133   2 260   2 204 133   1 204 133   3 260   1 260   5 194 133\n",
      "   4 201 133   0 260   1 198 133   3 260   1 260   5 196 133   4 260   0\n",
      " 202 133]\n",
      "[260   5 198 133   1 171 133   2 260   1 260   2 198 133   4 260   4 193\n",
      " 133   1 185 133   5 260   1 196 133   0 260   3 260   6 193 133   0 185\n",
      " 133   4 260   2 260   1 200 133   5 260   3 195 133   1 185 133   3 260\n",
      "   0 260   4 195 133   2 260   3 189 133   1 186 133   2 260   2 141 133\n",
      "   1 260   0 199 133   2 260   0 260  36 208 133   1 202 133   0 201 133\n",
      "   0 200 133   2 260   2 260   8 260   9 260  30 202 133   1 203 133   1\n",
      " 187 133   0 175 133   2 163 133   1 260   0 260   1 260   1 260   1 260\n",
      "   4 161 133   3 260   1 177 133   2 146 133   5 260   1 167 133   2 161\n",
      " 133   2 260   1 260   3 172 133   2 260   2 260   1 171 133   1 179 133\n",
      "   2 260   3 173 133   2 260   1 260   1 159 133   2 177 133   3 176 133\n",
      "   1 260   2 158 133   1 260   0 260   2 175 133   2 260   2 260   3 169\n",
      " 133   0 178 133   4 260   0 166 133   1 260   2 167 133   1 178 133   1\n",
      " 260   0 171 133   0 260   2 178 133   1 260   3 260   1 177 133   2 260\n",
      "   1 260   2 201 133   0 199 133   4 260   0 260   2 195 133   6 260   3\n",
      " 193 133   1 183 133   3 260   0 260   3 198 133   4 260   3 194 133   0\n",
      " 187 133   5 260   0 260   1 188 133   2 260   5 200 133   0 192 133   5\n",
      " 260   1 260   2 196 133   5 260   3 193 133   0 190 133   3 260   0 260\n",
      "   3 203 133   4 260   3 201 133   1 194 133   4 260   1 260   2 200 133\n",
      "   5 260   3 198 133   2 178 133   3 260   0 260   3 206 133   4 260   2\n",
      " 202 133   0 177 133   3 260   1 260   3 204 133   3 260  38 208 133   1\n",
      " 202 133   1 198 133   0 206 133   1 260   1 260   0 260   0 260   1 152\n",
      " 133   0 260   0 113 133   3 260  40 199 133   1 202 133   1 201 133   2\n",
      " 141 133   1 260   1 260   1 260   0 163 133   1 260   3 260   6 148 133\n",
      "   1 155 133   2 165 133   1 260   3 260   0 166 133   2 260   2 260   3\n",
      " 169 133   1 175 133   3 260   3 174 133   2 260   1 176 133   1 260   0\n",
      " 171 133   6 182 133   0 260   1 161 133   2 260   1 167 133   0 260   2\n",
      " 260   1 260   2 176 133   2 174 133   5 260   1 172 133   1 260   2 169\n",
      " 133   1 171 133   3 165 133   2 260   0 167 133   1 260   0 260   0 174\n",
      " 133   2 260   1 260   0 260   4 177 133   4 260   6 183 133   1 185 133\n",
      "   2 260   1 260   4 182 133   3 260   4 184 133   0 182 133   3 260   0\n",
      " 260   5 176 133   2 260   2 170 133   2 180 133   1 260   2 260   6 177\n",
      " 133   3 260   2 186 133   0 193 133   3 260   0 153 133   1 260   2 260\n",
      "   4 199 133   5 190 133   0 260   0 186 133   2 158 133   0 260   0 260\n",
      "   0 260   5 193 133   4 194 133   0 260   0 190 133   2 260   3 260   6\n",
      " 170 133   2 260   2 204 133   1 204 133   3 260   1 260   5 194 133   4\n",
      " 201 133   0 260   1 198 133   3 260   1 260   5 196 133   4 260   0 202\n",
      " 133   0]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_data(length=Time):\n",
    "    index = np.random.randint(0, len(data_files))\n",
    "    data = np.load(data_files[index])['eventlist']\n",
    "    \n",
    "    # time augmentation\n",
    "    data[:, 0] *= np.random.uniform(0.95, 1.05)\n",
    "    \n",
    "    # absolute time to relative interval\n",
    "    data[1:, 0] = data[1:, 0] - data[:-1, 0]\n",
    "    data[0, 0] = 0\n",
    "    \n",
    "    # discretize interval into IntervalDim\n",
    "    data[:, 0] = np.clip(np.round(data[:, 0] * IntervalDim), 0, IntervalDim - 1)\n",
    "    \n",
    "    # Note augmentation\n",
    "    data[:, 2] += np.random.randint(-6, 6)\n",
    "    data[:, 2] = np.clip(data[:, 2], 0, NoteOnDim)\n",
    "    \n",
    "    eventlist = []\n",
    "    for d in data:\n",
    "        # append interval\n",
    "        interval = d[0]\n",
    "        eventlist.append(interval)\n",
    "    \n",
    "        # note on case\n",
    "        if d[1] == 1:\n",
    "            velocity = d[3] + VelocityOffset\n",
    "            note = d[1] + NoteOnOffset\n",
    "            eventlist.append(velocity)\n",
    "            eventlist.append(note)\n",
    "            \n",
    "        # note off case\n",
    "        elif d[1] == 0:\n",
    "            note = d[1] + NoteOffOffset\n",
    "            eventlist.append(note)\n",
    "            \n",
    "    eventlist = np.array(eventlist).astype(np.int)\n",
    "    \n",
    "    if len(eventlist) > (length+1):\n",
    "        start_index = np.random.randint(0, len(eventlist) - (length+1))\n",
    "        eventlist = eventlist[start_index:start_index+(length+1)]\n",
    "        \n",
    "    # pad zeros\n",
    "    if len(eventlist) < (length+1):\n",
    "        pad = (length+1) - len(eventlist)\n",
    "        eventlist = np.pad(eventlist, (pad, 0), 'constant')\n",
    "        \n",
    "    x = eventlist[:length]\n",
    "    y = eventlist[1:length+1]\n",
    "    \n",
    "    return x, y\n",
    "    \n",
    "x, y = get_data()\n",
    "print('x shape : ', x.shape)\n",
    "print('y shape : ', y.shape)\n",
    "print(x)\n",
    "print(y)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# thanks to @openai: https://github.com/openai/gpt-2/blob/master/src/model.py\n",
    "def shape_list(x):\n",
    "    \"\"\"Deal with dynamic shape in tensorflow cleanly.\"\"\"\n",
    "    static = x.shape.as_list()\n",
    "    dynamic = tf.shape(x)\n",
    "    return [dynamic[i] if s is None else s for i, s in enumerate(static)]\n",
    "\n",
    "def model(inputs):\n",
    "    with tf.variable_scope('model'):\n",
    "        # inputs : [Batch, Time, EmbeddingDim]\n",
    "        \n",
    "        Batch, _, _ = shape_list(inputs)\n",
    "        \n",
    "        # [Heads, Batch, Time, HeadDim]\n",
    "        Q = tf.stack([tf.layers.dense(inputs, HeadDim, use_bias=False) for _ in range(Heads)])\n",
    "        K = tf.stack([tf.layers.dense(inputs, HeadDim, use_bias=False) for _ in range(Heads)])\n",
    "        V = tf.stack([tf.layers.dense(inputs, HeadDim, use_bias=False) for _ in range(Heads)])\n",
    "        \n",
    "        '''\n",
    "        [E_(-T+1), ..., E_0]\n",
    "        '''\n",
    "        E = tf.get_variable('E', [Heads, Time, HeadDim])\n",
    "        \n",
    "        # [Heads, Batch * Time, HeadDim]\n",
    "        Q_ = tf.reshape(Q, [Heads, Batch * Time, HeadDim])\n",
    "        \n",
    "        # [Heads, Batch * Time, Time]\n",
    "        S = tf.matmul(Q_, E, transpose_b=True)\n",
    "        # [Heads, Batch, Time, Time]\n",
    "        S = tf.reshape(S, [Heads, Batch, Time, Time])\n",
    "        # [Heads, Batch, Time, Time+1]\n",
    "        S = tf.pad(S, ((0, 0), (0, 0), (0, 0), (1, 0)))\n",
    "        # [Heads, Batch, Time+1, Time]\n",
    "        S = tf.reshape(S, [Heads, Batch, Time+1, Time])\n",
    "        # [Heads, Batch, Time, Time]\n",
    "        S = S[:, :, 1:]\n",
    "        \n",
    "        # [Heads, Batch, Time, Time]\n",
    "        attention = (tf.matmul(Q, K, transpose_b=True) + S) / np.sqrt(HeadDim)\n",
    "        \n",
    "        '''\n",
    "        # [Time, Time]  [[1 0 0 0]\n",
    "                         [1 1 0 0]\n",
    "                         [1 1 1 0]\n",
    "                         [1 1 1 1]]\n",
    "        '''\n",
    "        mask = tf.matrix_band_part(tf.ones([Time, Time]), -1, 0)\n",
    "        \n",
    "        # [Heads, Batch, Time, Time]\n",
    "        attention = attention * mask - tf.cast(1e10, attention.dtype) * (1-mask)\n",
    "        # [Heads, Batch, Time, Time]\n",
    "        score = tf.nn.softmax(attention, axis=3)\n",
    "        \n",
    "        # [Heads, Batch, Time, HeadDim]\n",
    "        context = tf.matmul(score, V)\n",
    "        # [Batch, Time, Heads, HeadDim]\n",
    "        context = tf.transpose(context, [1, 2, 0, 3])\n",
    "        # [Batch, Time, ContextDim]\n",
    "        context = tf.reshape(context, [Batch, Time, ContextDim])\n",
    "        # [Batch, Time, ContextDim]\n",
    "        context = tf.layers.dense(context, ContextDim, tf.nn.relu)\n",
    "        # [Batch, Time, EventDim]\n",
    "        logits = tf.layers.dense(context, EventDim)\n",
    "        \n",
    "        return logits\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Draw Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.reset_default_graph()\n",
    "\n",
    "'''INPUTS'''\n",
    "# [Batch, Time]\n",
    "inputs = tf.placeholder(dtype=tf.int32, shape=[None, Time])\n",
    "# [Batch, Time]\n",
    "targets = tf.placeholder(dtype=tf.int32, shape=[None, None])\n",
    "\n",
    "# [Batch, Time, EventDim], for the use of visualization\n",
    "inputs_onehot = tf.one_hot(inputs, axis=2, depth=EventDim)\n",
    "\n",
    "'''EMBEDDING'''\n",
    "embedding = tf.get_variable('embedding', [EventDim, EmbeddingDim])\n",
    "# [Batch, Time, EventDim]\n",
    "inputs_embedding = tf.gather(embedding, inputs)\n",
    "\n",
    "'''GET LOGITS'''\n",
    "# [Batch, Time, EventDim]\n",
    "logits = model(inputs_embedding)\n",
    "\n",
    "# for the use of visualization\n",
    "probs = tf.nn.softmax(logits, axis=2)\n",
    "\n",
    "'''LOSS'''\n",
    "cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=targets, logits=logits)\n",
    "loss = tf.reduce_mean(cross_entropy)\n",
    "\n",
    "'''SAMPLING'''\n",
    "dist = tf.distributions.Categorical(logits=logits[:, -1])\n",
    "# [Batch, 1]\n",
    "sample = dist.sample()\n",
    "\n",
    "'''TRAIN'''\n",
    "global_step = tf.Variable(0, name='global_step')\n",
    "learning_rate = tf.Variable(1e-3, name='learning_rate')\n",
    "train_step = tf.train.AdamOptimizer(learning_rate).minimize(loss, global_step)\n",
    "\n",
    "'''SESSION OPEN'''\n",
    "\n",
    "sess_config = tf.ConfigProto()\n",
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.9, visible_device_list= '0')\n",
    "sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "print('graph created')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dir = 'save/music-transformer'\n",
    "save_dir = 'save/music-transformer'\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "if True:\n",
    "    restore_file = tf.train.latest_checkpoint(load_dir)\n",
    "    if restore_file is not None:\n",
    "        saver.restore(sess, restore_file)\n",
    "        print(\"Model restored.\")\n",
    "    else:\n",
    "        print('model not exist.')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "class Logger(SummaryWriter):\n",
    "    def __init__(self, logdir):\n",
    "        super(Logger, self).__init__(logdir)\n",
    "\n",
    "    def log(self, log_string, value, iteration):\n",
    "            self.add_scalar(log_string, value, iteration)\n",
    "            \n",
    "logger = Logger(save_dir)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa.display\n",
    "from time import sleep\n",
    "import time\n",
    "\n",
    "batch_size = 64\n",
    "def get_batch_data(batch_size, time):\n",
    "    _inputs = []\n",
    "    _targets = []\n",
    "    for _ in range(batch_size):\n",
    "        x, y = get_data(time)\n",
    "        _inputs.append(x)\n",
    "        _targets.append(y)\n",
    "        \n",
    "    _inputs = np.stack(_inputs)\n",
    "    _targets = np.stack(_targets)\n",
    "    \n",
    "    return _inputs, _targets\n",
    "    \n",
    "while(True):\n",
    "    for _ in range(100):\n",
    "        _inputs, _targets = get_batch_data(batch_size, Time)\n",
    "        print(_inputs.shape, _targets.shape)\n",
    "        \n",
    "        _, _global_step, _loss = sess.run([train_step, global_step, loss], \n",
    "                                          feed_dict={inputs: _inputs, \n",
    "                                                     targets: _targets,\n",
    "                                                     learning_rate: 1e-3})\n",
    "        print('step : ', _global_step, 'loss : ', _loss)\n",
    "        \n",
    "        if _global_step % 10 == 0:\n",
    "            logger.log('loss', _loss, _global_step)\n",
    "        \n",
    "        if _global_step % 1000 == 0:\n",
    "            save_path = saver.save(sess, save_dir + '/checkpoint', global_step=_global_step)\n",
    "            print(\"Model saved in path: %s\" % save_path)\n",
    "        \n",
    "    clear_output()\n",
    "    \n",
    "    _inputs_onehot, _probs, _h = sess.run([inputs_onehot, probs, h], feed_dict={inputs: _inputs})\n",
    "    \n",
    "    plt.figure(figsize=[18, 10])\n",
    "    librosa.display.specshow(_inputs_onehot[0].T)\n",
    "    plt.show()\n",
    "    \n",
    "    plt.figure(figsize=[18, 10])\n",
    "    librosa.display.specshow(_probs[0].T)\n",
    "    plt.show()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
